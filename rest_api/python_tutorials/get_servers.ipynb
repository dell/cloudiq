{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Required libs to measure / track the time to do things\n",
    "import time # for some performance testing\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Required libs to authenticate via OAuth Client Credentials and fetch the data from CloudIQ REST API\n",
    "from oauthlib.oauth2 import BackendApplicationClient\n",
    "from requests_oauthlib import OAuth2Session\n",
    "\n",
    "# Required libs to process the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Required libs to visualize\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CloudIQ REST API endpoints\n",
    "CIQ_TOKEN_URL = 'https://cloudiq.apis.dell.com/auth/oauth/v2/token'\n",
    "CIQ_BASE_API_URL = 'https://cloudiq.apis.dell.com/cloudiq/rest/v1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't want your API credentials to leak, do you?\n",
    "# just create a \".env\" file in the same directory as the Jupyter notebooks, with those 2 variables defined:\n",
    "# CLIENT_ID=<your_client_id>\n",
    "# CLIENT_SECRET=<your_client_secret>\n",
    "load_dotenv()\n",
    "\n",
    "client_id = os.getenv('CLIENT_ID')\n",
    "client_secret = os.getenv('CLIENT_SECRET')\n",
    "\n",
    "print('OAuth2 Client Credentials ID used:', client_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4edd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication to CloudIQ REST API\n",
    "client = BackendApplicationClient(client_id=client_id)\n",
    "oauth = OAuth2Session(client=client)\n",
    "token = oauth.fetch_token(token_url=CIQ_TOKEN_URL, client_id=client_id, client_secret=client_secret)\n",
    "\n",
    "print('OAuth2 Token type:', token['token_type'], '- Expires in:', token['expires_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f8a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching all servers\n",
    "# A simple REST API call to CloudIQ REST API, adding the resource name to the prefix, and passing some parameters to filter instances, and to select attributes\n",
    "params = { \n",
    "#     'filter': \"country eq 'US'\",\n",
    "    'select': 'system_id,country,state,city,site_name,system,version,model,system_tags,power_state,power_consumption,health_score,inlet_temperature,cpu_usage_percent,memory_usage_percent'\n",
    "}\n",
    "\n",
    "r = oauth.get(CIQ_BASE_API_URL + 'server_systems', params=params)\n",
    "print('Status Code:', r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many instances / servers do we have?\n",
    "print('Number of instances:', r.json()['paging']['total_instances'])\n",
    "# r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame\n",
    "df_object = pd.json_normalize(r.json(), record_path =['results'])\n",
    "\n",
    "print('Dataframe size:', df_object.shape)\n",
    "# df_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering systems with no data\n",
    "df_object[~df_object['cpu_usage_percent'].isnull() & \\\n",
    "          ~df_object['inlet_temperature'].isnull() & \\\n",
    "          ~df_object['memory_usage_percent'].isnull() & \\\n",
    "          ~df_object['power_consumption'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching some metrics - functions\n",
    "MAX_ID = 5 # Not all resource ids at the same time, but in chunks\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "# Transforming the metrics data into a DataFrame\n",
    "def flatten(s):\n",
    "    metrics = s['metrics']\n",
    "    results = s['results']\n",
    "    \n",
    "    # Creating a record per object, per metric and per timestamp to then create a DataFrame\n",
    "    data = [{'object': metric['id'], 'datetime': ts['timestamp']} | dict(zip(metrics, ts['values']))\n",
    "            for metric in results\n",
    "            for ts in metric['timestamps']]\n",
    "    \n",
    "    df = pd.DataFrame.from_records(data)\n",
    "    if not df.empty:\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Crafting the POST query\n",
    "def post_metric_json(from_dt, to_dt, ids, resource, metrics, interval):\n",
    "    return {\n",
    "        'from': from_dt.isoformat(),\n",
    "        'to': to_dt.isoformat(),\n",
    "        'resource_type': resource,\n",
    "        'ids': ids,\n",
    "        'interval': interval,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    \n",
    "# Fetching timeseries (values over time) / metrics between 2 dates\n",
    "def get_metrics_df(from_dt, to_dt, ids, resource, metrics, interval):\n",
    "    # Start with an empty dataframe\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    with tqdm(total=len(ids)) as pbar:\n",
    "        # Split the request into sub-requests (resource ids)\n",
    "        for ids_subset in chunks(ids, MAX_ID):\n",
    "            # Set the from and to, maybe we can fetch all data in 1 call / response\n",
    "            from_dt_post = from_dt\n",
    "            to_dt_post = to_dt\n",
    "            \n",
    "            # Send requests until we have all content - i.e. until no more HTTP 206 / Partial Content\n",
    "            nb_request = 0\n",
    "            while True:\n",
    "                json_to_post = post_metric_json(from_dt_post, to_dt_post, ids_subset, resource, metrics, interval)\n",
    "                nb_request += 1\n",
    "\n",
    "                # Post the query to fetch the metrics\n",
    "                r = oauth.post(CIQ_BASE_API_URL + 'metrics/query', json=json_to_post)\n",
    "                # Append response to dataframe\n",
    "                new_data = flatten(r.json())\n",
    "                data = pd.concat([data, new_data])\n",
    "                # If no more data, stop here\n",
    "                if r.status_code == 200:\n",
    "                    break\n",
    "                # If Partial content, continue by shifting the from date to the one received in the response\n",
    "                elif r.status_code == 206:\n",
    "                    if not new_data.empty:\n",
    "                        from_dt_post = max(new_data['datetime'])\n",
    "                    # Stop if the response returns no data\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            # print('Number of API calls:', nb_request)\n",
    "            # print('Updated dataframe size:', data.shape)\n",
    "            pbar.update(len(ids_subset))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59af0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching some metrics over time\n",
    "# between 25 days ago and now\n",
    "TO = datetime.now(timezone.utc).replace(microsecond=0)\n",
    "FROM = TO - timedelta(days=25)\n",
    "print('From:', FROM)\n",
    "print('To:', TO)\n",
    "\n",
    "RESOURCE = 'server_system'\n",
    "\n",
    "METRICS = [\n",
    "        'inlet_temperature',\n",
    "        'power_consumption',\n",
    "        'system_energy',\n",
    "        'cpu_usage_percent'\n",
    "]\n",
    "\n",
    "INTERVAL = 'PT15M'\n",
    "\n",
    "# for all servers which have a temperature at the system's level\n",
    "SERVERS = df_object[~df_object['inlet_temperature'].isnull()]['id'].tolist()\n",
    "\n",
    "start = time.time()\n",
    "df_all_metrics_data = get_metrics_df(FROM, TO, SERVERS, RESOURCE, METRICS, INTERVAL)\n",
    "end = time.time()\n",
    "\n",
    "print('Dataframe size:', df_all_metrics_data.shape)\n",
    "print('Time (ms):', (end-start)*1000)\n",
    "# df_all_metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05070d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge metrics with metadata / system level attributes\n",
    "df_all_metrics_data = pd.merge(df_all_metrics_data, df_object[['system', 'country', 'city', 'site_name']], how='left', left_on='object', right_on='system')\n",
    "df_all_metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2abfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treemap of the temperature per system\n",
    "fig = px.treemap(df_object, path=[px.Constant(\"ALL\"), 'country', 'city', 'site_name', 'system'], values='inlet_temperature',\n",
    "                    color='inlet_temperature', hover_data=['id', 'city'],\n",
    "                    custom_data=df_object[['country', 'city', 'site_name']],\n",
    "                    color_continuous_scale='Jet')\n",
    "fig.update_traces(root_color='lightgrey', hovertemplate='Server: %{label}<br>Temperature: %{color}<extra></extra><br><br>Site: %{customdata[0]} / %{customdata[1]}')\n",
    "fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the temperature - ToDo: per group...\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        hovertemplate='Server: %{y}<br>Date: %{x}<br>Temperature: %{z}<extra></extra><br><br>Site: %{customdata}',\n",
    "        z=df_all_metrics_data['inlet_temperature'],\n",
    "        x=df_all_metrics_data['datetime'],\n",
    "        y=df_all_metrics_data['object'],\n",
    "        customdata=df_all_metrics_data['city'],\n",
    "        colorscale='RdBu_r')) # RdBu_r OR Jet\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Temperature over time',\n",
    "    xaxis_nticks=24,\n",
    "    height=800)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95408fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line chart with 2 metrics (Power, CPU)\n",
    "# Create figure with 2 Y axis\n",
    "# Specify the filter to get the specific server you want to focus on:\n",
    "object_filter = df_all_metrics_data['object'] == '<a_server>'\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_all_metrics_data[object_filter]['datetime'], y=df_all_metrics_data[object_filter]['power_consumption'], name='Power (W)', yaxis='y')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_all_metrics_data[object_filter]['datetime'], y=df_all_metrics_data[object_filter]['cpu_usage_percent'], name='CPU Utilization (%)', yaxis='y2')\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"CPU Utilization vs Power consumption\"\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date and time\")\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "\n",
    "fig.update_yaxes(title_text=\"<b>Power (W)</b>\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"<b>CPU Utilization (%)</b>\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
